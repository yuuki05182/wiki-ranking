import requests
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from collections import defaultdict

now_jst = datetime.now(ZoneInfo("Asia/Tokyo"))
timestamp = now_jst.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')

# User-Agentï¼ˆWikimediaã®ãƒãƒªã‚·ãƒ¼ã«æº–æ‹ ï¼‰
HEADERS = {
    'User-Agent': 'YukiBot/1.0 (https://github.com/yimam)'  # ã”è‡ªèº«ã®URLã‚„é€£çµ¡å…ˆã«å¤‰æ›´ã—ã¦ãã ã•ã„
}

# é™¤å¤–ã™ã‚‹è¨˜äº‹å
EXCLUDE_TITLES = ['Main_Page', 'ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸', 'ç‰¹åˆ¥:æ¤œç´¢']

# æ—¥æœ¬èªã®æ›œæ—¥ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def format_date_with_weekday(date):
    weekdays = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
    return date.strftime('%Y/%m/%d') + f'ï¼ˆ{weekdays[date.weekday()]}ï¼‰'

def format_period_with_weekday(start_date):
    weekdays = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
    start_jst = datetime.combine(start_date, datetime.min.time()) + timedelta(hours=9)
    end_jst = start_jst + timedelta(days=1) - timedelta(minutes=1)
    start_label = f"{start_jst.strftime('%Yå¹´%mæœˆ%dæ—¥')}ï¼ˆ{weekdays[start_jst.weekday()]}ï¼‰"
    end_label = f"{end_jst.strftime('%Yå¹´%mæœˆ%dæ—¥')}ï¼ˆ{weekdays[end_jst.weekday()]}ï¼‰"
    return f"{start_label}9æ™‚00åˆ†ã‹ã‚‰{end_label}8æ™‚59åˆ†"

def format_period(start_date):
    start_jst = datetime.combine(start_date, datetime.min.time()) + timedelta(hours=9)
    end_jst = start_jst + timedelta(days=1) - timedelta(minutes=1)
    return f"{start_jst.strftime('%Yå¹´%mæœˆ%dæ—¥')}9æ™‚00åˆ†ã‹ã‚‰{end_jst.strftime('%Yå¹´%mæœˆ%dæ—¥')}8æ™‚59åˆ†"

def get_topviews(date_str, project='ja.wikipedia.org'):
    url = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/top/{project}/all-access/{date_str}'
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f'Error: {response.status_code} on {date_str}')
        return []
    data = response.json()
    articles = data.get('items', [])[0].get('articles', [])
    return [a for a in articles if a['article'] not in EXCLUDE_TITLES]

def accumulate_views(start_date, end_date):
    view_counts = defaultdict(int)
    delta = (end_date - start_date).days + 1
    for i in range(delta):
        date = start_date + timedelta(days=i)
        date_str = date.strftime('%Y/%m/%d')
        articles = get_topviews(date_str)
        for a in articles:
            view_counts[a['article']] += a['views']
    return sorted(view_counts.items(), key=lambda x: x[1], reverse=True)[:10]

# æ—¥æœ¬æ™‚é–“ã§ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
today = datetime.now(ZoneInfo("Asia/Tokyo")).date()

# ğŸ“… ä¸€æ˜¨æ—¥ï¼ˆå–å¾—å¯èƒ½ãªæœ€æ–°æ—¥ï¼‰
latest_date = today - timedelta(days=2)
latest_str = format_date_with_weekday(latest_date)

# ğŸ“… ä¸€æ˜¨æ—¥
latest_articles = get_topviews(latest_date.strftime('%Y/%m/%d'))
yesterday_ranking = [
    {'title': a['article'], 'views': a['views']}
    for a in latest_articles[:10]
]
print(f"\nğŸ“… {format_period_with_weekday(latest_date)} ã®ãƒˆãƒƒãƒ—è¨˜äº‹:")
for i, a in enumerate(yesterday_ranking, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… 3æ—¥å‰
day_before_yesterday = latest_date - timedelta(days=1)
day_before_yesterday_str = format_date_with_weekday(day_before_yesterday)
articles_day_before_yesterday = get_topviews(day_before_yesterday.strftime('%Y/%m/%d'))
ranking_day_before_yesterday = [
    {'title': a['article'], 'views': a['views']}
    for a in articles_day_before_yesterday[:10]
]
print(f"\nğŸ“… {format_period_with_weekday(day_before_yesterday)} ã®ãƒˆãƒƒãƒ—è¨˜äº‹:")
for i, a in enumerate(ranking_day_before_yesterday, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… éå»7æ—¥é–“ï¼ˆæ˜¨æ—¥ã‚’å«ã‚€ï¼‰
start_7 = latest_date - timedelta(days=6)
end_7 = latest_date
views_7 = accumulate_views(start_7, end_7)
ranking_7 = [{'title': t, 'views': v} for t, v in views_7]
period_7 = f"{format_period_with_weekday(start_7).split('ã‹ã‚‰')[0]}ã‹ã‚‰{format_period_with_weekday(end_7).split('ã‹ã‚‰')[1]}"
print(f"\nğŸ“… {period_7} ã®ç´¯è¨ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
for i, a in enumerate(ranking_7, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… 9æ—¥å‰ã€œ15æ—¥å‰ï¼ˆéå»7æ—¥é–“ã®é–‹å§‹æ—¥ã®å‰æ—¥ã‹ã‚‰7æ—¥é–“ï¼‰
end_8_14 = start_7 - timedelta(days=1)
start_8_14 = end_8_14 - timedelta(days=6)
views_8_14 = accumulate_views(start_8_14, end_8_14)
ranking_8_14 = [{'title': t, 'views': v} for t, v in views_8_14]
period_8_14 = f"{format_period_with_weekday(start_8_14).split('ã‹ã‚‰')[0]}ã‹ã‚‰{format_period_with_weekday(end_8_14).split('ã‹ã‚‰')[1]}"
print(f"\nğŸ“… {period_8_14} ã®ç´¯è¨ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
for i, a in enumerate(ranking_8_14, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… éå»30æ—¥é–“ï¼ˆæ˜¨æ—¥ã‚’å«ã‚€ï¼‰
start_30 = latest_date - timedelta(days=29)
end_30 = latest_date
views_30 = accumulate_views(start_30, end_30)
ranking_30 = [{'title': t, 'views': v} for t, v in views_30]
period_30 = f"{format_period_with_weekday(start_30).split('ã‹ã‚‰')[0]}ã‹ã‚‰{format_period_with_weekday(end_30).split('ã‹ã‚‰')[1]}"
print(f"\nğŸ“… {period_30} ã®ç´¯è¨ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
for i, a in enumerate(ranking_30, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# JSONä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆæ›œæ—¥ä»˜ãï¼‰
output = {
    format_period_with_weekday(latest_date): {
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': yesterday_ranking
    },
    format_period_with_weekday(day_before_yesterday): {
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_day_before_yesterday
    },
    f"{format_period_with_weekday(start_7).split('ã‹ã‚‰')[0]}ã‹ã‚‰{format_period_with_weekday(end_7).split('ã‹ã‚‰')[1]}": {
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_7
    },
    f"{format_period_with_weekday(start_8_14).split('ã‹ã‚‰')[0]}ã‹ã‚‰{format_period_with_weekday(end_8_14).split('ã‹ã‚‰')[1]}": {
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_8_14
    },
    f"{format_period_with_weekday(start_30).split('ã‹ã‚‰')[0]}ã‹ã‚‰{format_period_with_weekday(end_30).split('ã‹ã‚‰')[1]}": {
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_30
    }
}

output['æ›´æ–°æ™‚åˆ»'] = timestamp

print(f"\nğŸ“… æ›´æ–°æ™‚åˆ»ï¼ˆJSTï¼‰: {timestamp}")

import os

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªèº«ã®å ´æ‰€ã‚’å–å¾—ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰
script_dir = os.path.dirname(os.path.abspath(__file__))

# ä¿å­˜å…ˆã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã«å›ºå®š
save_path = os.path.join(script_dir, 'docs', 'ranking.json')

# ä¿å­˜å‡¦ç†
with open(save_path, 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

# script.js ã‚’ docs ã«ã‚³ãƒ”ãƒ¼
import shutil
source_js = os.path.join(script_dir, 'script.js')
target_js = os.path.join(script_dir, 'docs', 'script.js')
os.makedirs(os.path.dirname(target_js), exist_ok=True)
shutil.copy2(source_js, target_js)
print(f"âœ… script.js ã‚’ docs ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã—ãŸï¼š{target_js}")

# ä¿å­˜å¾Œã«ç¢ºèª
with open(save_path, 'r', encoding='utf-8') as f:
    preview = json.load(f)
    print("\nâœ… ä¿å­˜ã•ã‚ŒãŸæ›´æ–°æ™‚åˆ»:", preview.get('æ›´æ–°æ™‚åˆ»', 'è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'))

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜éšå±¤ã«ã‚ã‚‹ script.js ã‚’å‰Šé™¤
script_path_to_delete = os.path.join(script_dir, 'script.js')

if os.path.exists(script_path_to_delete):
    os.remove(script_path_to_delete)
    print(f"ğŸ—‘ï¸ script.js ã‚’å‰Šé™¤ã—ã¾ã—ãŸï¼š{script_path_to_delete}")
else:
    print("âš ï¸ å‰Šé™¤å¯¾è±¡ã® script.js ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã™ã§ã«å‰Šé™¤æ¸ˆã¿ã‹å­˜åœ¨ã—ãªã„å¯èƒ½æ€§ï¼‰")


# ä¿å­˜å¾Œã«ç¢ºèª
with open(save_path, 'r', encoding='utf-8') as f:
    preview = json.load(f)
    print("\nâœ… ä¿å­˜ã•ã‚ŒãŸæ›´æ–°æ™‚åˆ»:", preview.get('æ›´æ–°æ™‚åˆ»', 'è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'))