import requests
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from collections import defaultdict

# User-Agentï¼ˆWikimediaã®ãƒãƒªã‚·ãƒ¼ã«æº–æ‹ ï¼‰
HEADERS = {
    'User-Agent': 'YukiBot/1.0 (https://github.com/yimam)'  # ã”è‡ªèº«ã®URLã‚„é€£çµ¡å…ˆã«å¤‰æ›´ã—ã¦ãã ã•ã„
}

# é™¤å¤–ã™ã‚‹è¨˜äº‹å
EXCLUDE_TITLES = ['Main_Page', 'ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸', 'ç‰¹åˆ¥:æ¤œç´¢']

# æ—¥æœ¬èªã®æ›œæ—¥ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def format_date_with_weekday(date):
    weekdays = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
    return date.strftime('%Y/%m/%d') + f'ï¼ˆ{weekdays[date.weekday()]}ï¼‰'

def get_topviews(date_str, project='ja.wikipedia.org'):
    url = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/top/{project}/all-access/{date_str}'
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f'Error: {response.status_code} on {date_str}')
        return []
    data = response.json()
    articles = data.get('items', [])[0].get('articles', [])
    return [a for a in articles if a['article'] not in EXCLUDE_TITLES]

def accumulate_views(start_date, end_date):
    view_counts = defaultdict(int)
    delta = (end_date - start_date).days + 1
    for i in range(delta):
        date = start_date + timedelta(days=i)
        date_str = date.strftime('%Y/%m/%d')
        articles = get_topviews(date_str)
        for a in articles:
            view_counts[a['article']] += a['views']
    return sorted(view_counts.items(), key=lambda x: x[1], reverse=True)[:10]

# æ—¥æœ¬æ™‚é–“ã§ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
today = datetime.now(ZoneInfo("Asia/Tokyo")).date()

# ğŸ“… ä¸€æ˜¨æ—¥ï¼ˆå–å¾—å¯èƒ½ãªæœ€æ–°æ—¥ï¼‰
latest_date = today - timedelta(days=2)
latest_str = format_date_with_weekday(latest_date)

# ğŸ“… ä¸€æ˜¨æ—¥
latest_articles = get_topviews(latest_date.strftime('%Y/%m/%d'))
yesterday_ranking = [
    {'title': a['article'], 'views': a['views']}
    for a in latest_articles[:10]
]
print(f"\nğŸ“… ä¸€æ˜¨æ—¥ï¼ˆ{latest_str}ï¼‰ã®ãƒˆãƒƒãƒ—è¨˜äº‹:")
for i, a in enumerate(yesterday_ranking, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… 3æ—¥å‰
day_before_yesterday = latest_date - timedelta(days=1)
day_before_yesterday_str = format_date_with_weekday(day_before_yesterday)
articles_day_before_yesterday = get_topviews(day_before_yesterday.strftime('%Y/%m/%d'))
ranking_day_before_yesterday = [
    {'title': a['article'], 'views': a['views']}
    for a in articles_day_before_yesterday[:10]
]
print(f"\nğŸ“… 3æ—¥å‰ï¼ˆ{day_before_yesterday_str}ï¼‰ã®ãƒˆãƒƒãƒ—è¨˜äº‹:")
for i, a in enumerate(ranking_day_before_yesterday, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… éå»7æ—¥é–“ï¼ˆæ˜¨æ—¥ã‚’å«ã‚€ï¼‰
start_7 = latest_date - timedelta(days=6)
end_7 = latest_date
views_7 = accumulate_views(start_7, end_7)
ranking_7 = [{'title': t, 'views': v} for t, v in views_7]
print(f"\nğŸ“… éå»7æ—¥é–“ï¼ˆ{format_date_with_weekday(start_7)}ã€œ{format_date_with_weekday(end_7)}ï¼‰ã®ç´¯è¨ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
for i, a in enumerate(ranking_7, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… 9æ—¥å‰ã€œ15æ—¥å‰ï¼ˆéå»7æ—¥é–“ã®é–‹å§‹æ—¥ã®å‰æ—¥ã‹ã‚‰7æ—¥é–“ï¼‰
end_8_14 = start_7 - timedelta(days=1)
start_8_14 = end_8_14 - timedelta(days=6)
views_8_14 = accumulate_views(start_8_14, end_8_14)
ranking_8_14 = [{'title': t, 'views': v} for t, v in views_8_14]
print(f"\nğŸ“… 9æ—¥å‰ã€œ15æ—¥å‰ï¼ˆ{format_date_with_weekday(start_8_14)}ã€œ{format_date_with_weekday(end_8_14)}ï¼‰ã®ç´¯è¨ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
for i, a in enumerate(ranking_8_14, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# ğŸ“… éå»30æ—¥é–“ï¼ˆæ˜¨æ—¥ã‚’å«ã‚€ï¼‰
start_30 = latest_date - timedelta(days=29)
end_30 = latest_date
views_30 = accumulate_views(start_30, end_30)
ranking_30 = [{'title': t, 'views': v} for t, v in views_30]
print(f"\nğŸ“… éå»30æ—¥é–“ï¼ˆ{format_date_with_weekday(start_30)}ã€œ{format_date_with_weekday(end_30)}ï¼‰ã®ç´¯è¨ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
for i, a in enumerate(ranking_30, 1):
    print(f"{i}. {a['title']} - {a['views']} views")

# JSONä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆæ›œæ—¥ä»˜ãï¼‰
output = {
    'ä¸€æ˜¨æ—¥': {
        'æ—¥ä»˜': format_date_with_weekday(latest_date),
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': yesterday_ranking
    },
    '3æ—¥å‰': {
        'æ—¥ä»˜': format_date_with_weekday(day_before_yesterday),
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_day_before_yesterday
    },
    'éå»7æ—¥é–“': {
        'é–‹å§‹æ—¥': format_date_with_weekday(start_7),
        'çµ‚äº†æ—¥': format_date_with_weekday(end_7),
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_7
    },
    '9æ—¥å‰ã€œ15æ—¥å‰': {
        'é–‹å§‹æ—¥': format_date_with_weekday(start_8_14),
        'çµ‚äº†æ—¥': format_date_with_weekday(end_8_14),
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_8_14
    },
    'éå»30æ—¥é–“': {
        'é–‹å§‹æ—¥': format_date_with_weekday(start_30),
        'çµ‚äº†æ—¥': format_date_with_weekday(end_30),
        'ãƒ©ãƒ³ã‚­ãƒ³ã‚°': ranking_30
    }
}

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
with open('ranking.json', 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=2)
